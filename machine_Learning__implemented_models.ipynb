{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyaZOLvG5uhttQnf/3IuOE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tmhieul/Boolean-Calculator/blob/master/machine_Learning__implemented_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Introduction**\n",
        "\n",
        "\n",
        "This report presents the findings of machine learning implementations for two distinct tasks: regression and classification. The primary aim is to predict housing prices using the California Housing dataset for the regression task and to predict survival outcomes for passengers aboard the Titanic in the classification task. For each task, various machine learning models have been employed, including baseline models for comparison. The report provides a detailed overview of the methodologies employed, including preprocessing steps, model selection rationale, and evaluation metrics. By analyzing the performance of different models, we aim to identify the most effective approach for each task. This report offers insights into the predictive capabilities of different machine learning algorithms and their suitability for specific prediction tasks.\n",
        "\n",
        "# **Part 2: Regression**\n",
        "\n",
        "\n",
        "Part 2-1: Pre-processing\n",
        "\n",
        "\n",
        "We'll begin by loading the California Housing dataset and splitting it into training and test sets. Then, we'll preprocess the data by handling missing values, encoding categorical variables, and scaling numerical features."
      ],
      "metadata": {
        "id": "pnm_EyHolIo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load California Housing dataset\n",
        "california_housing = pd.read_csv('/content/housing_coursework_entire_dataset_23-24.csv')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(california_housing.head())\n",
        "\n",
        "# Split features and target variable\n",
        "X_reg = california_housing.drop(columns=['median_house_value'])\n",
        "y_reg = california_housing['median_house_value']\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "# Handle categorical feature 'ocean_proximity'\n",
        "ocean_proximity_encoder = OneHotEncoder(sparse=False)\n",
        "X_train_reg_ocean_encoded = ocean_proximity_encoder.fit_transform(X_train_reg[['ocean_proximity']])\n",
        "X_test_reg_ocean_encoded = ocean_proximity_encoder.transform(X_test_reg[['ocean_proximity']])\n",
        "\n",
        "# Impute missing values for numerical features\n",
        "num_features = X_train_reg.select_dtypes(include=np.number).columns\n",
        "imputer_reg = SimpleImputer(strategy='median')\n",
        "X_train_reg_imputed = imputer_reg.fit_transform(X_train_reg[num_features])\n",
        "X_test_reg_imputed = imputer_reg.transform(X_test_reg[num_features])\n",
        "\n",
        "# Concatenate imputed numerical features with encoded categorical features\n",
        "X_train_reg_processed = np.concatenate([X_train_reg_imputed, X_train_reg_ocean_encoded], axis=1)\n",
        "X_test_reg_processed = np.concatenate([X_test_reg_imputed, X_test_reg_ocean_encoded], axis=1)\n",
        "\n",
        "# Main model: Random Forest Regression\n",
        "rf_reg_main = RandomForestRegressor()\n",
        "rf_reg_main.fit(X_train_reg_processed, y_train_reg)\n",
        "\n",
        "# Baseline models\n",
        "linear_reg_baseline = LinearRegression()\n",
        "linear_reg_baseline.fit(X_train_reg_processed, y_train_reg)\n",
        "\n",
        "svr_reg_baseline = SVR()\n",
        "svr_reg_baseline.fit(X_train_reg_processed, y_train_reg)\n",
        "\n",
        "# Evaluate the models\n",
        "mse_rf_reg_main = mean_squared_error(y_test_reg, rf_reg_main.predict(X_test_reg_processed))\n",
        "mse_linear_reg_baseline = mean_squared_error(y_test_reg, linear_reg_baseline.predict(X_test_reg_processed))\n",
        "mse_svr_reg_baseline = mean_squared_error(y_test_reg, svr_reg_baseline.predict(X_test_reg_processed))\n",
        "\n",
        "print(\"Random Forest Regression (Main Model) Mean Squared Error:\", mse_rf_reg_main)\n",
        "print(\"Linear Regression (Baseline Model) Mean Squared Error:\", mse_linear_reg_baseline)\n",
        "print(\"Support Vector Regression (Baseline Model) Mean Squared Error:\", mse_svr_reg_baseline)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edC8VIH4lL7Z",
        "outputId": "0e7f1693-1e20-4fb8-c53c-244f059f7809"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   No.  longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
            "0    1    -122.12     37.70                  17         2488           617.0   \n",
            "1    2    -122.21     38.10                  36         3018           557.0   \n",
            "2    3    -122.22     38.11                  43         1939           353.0   \n",
            "3    4    -122.20     37.78                  52         2300           443.0   \n",
            "4    5    -122.19     37.79                  50          954           217.0   \n",
            "\n",
            "   population  households  median_income  median_house_value ocean_proximity  \n",
            "0        1287         538         2.9922              179900        NEAR BAY  \n",
            "1        1445         556         3.8029              129900        NEAR BAY  \n",
            "2         968         392         3.1848              112700        NEAR BAY  \n",
            "3        1225         423         3.5398              158400        NEAR BAY  \n",
            "4         546         201         2.6667              172800        NEAR BAY  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Regression (Main Model) Mean Squared Error: 3556036069.8313823\n",
            "Linear Regression (Baseline Model) Mean Squared Error: 3883422015.410895\n",
            "Support Vector Regression (Baseline Model) Mean Squared Error: 12812942703.265884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2-2: Methodology\n",
        "For the regression task, we'll choose the Random Forest Regression model as the main model due to its ability to handle complex relationships and feature interactions in the data. Random Forest Regression works by building multiple decision trees and averaging their predictions to reduce overfitting and improve generalization."
      ],
      "metadata": {
        "id": "ekml_knklOUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main model: Random Forest Regression\n",
        "rf_reg_main = RandomForestRegressor()\n"
      ],
      "metadata": {
        "id": "AzGzUwhvlQFa"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2-3: Experiment**\n",
        "\n",
        "We'll compare the performance of the Random Forest Regression model with two baseline models: Linear Regression and Support Vector Regression. We'll evaluate the models using Mean Squared Error (MSE) as the evaluation metric."
      ],
      "metadata": {
        "id": "2JxoznnmlsS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline models\n",
        "lr_reg_baseline = LinearRegression()\n",
        "svr_reg_baseline = SVR()\n",
        "\n",
        "# Train the models\n",
        "rf_reg_main.fit(X_train_reg_processed, y_train_reg)\n",
        "lr_reg_baseline.fit(X_train_reg_processed, y_train_reg)\n",
        "svr_reg_baseline.fit(X_train_reg_processed, y_train_reg)\n",
        "\n",
        "# Evaluate the models\n",
        "mse_rf_reg_main = mean_squared_error(y_test_reg, rf_reg_main.predict(X_test_reg_processed))\n",
        "mse_lr_reg_baseline = mean_squared_error(y_test_reg, lr_reg_baseline.predict(X_test_reg_processed))\n",
        "mse_svr_reg_baseline = mean_squared_error(y_test_reg, svr_reg_baseline.predict(X_test_reg_processed))\n",
        "\n",
        "print(\"Random Forest Regression (Main Model) Mean Squared Error:\", mse_rf_reg_main)\n",
        "print(\"Linear Regression (Baseline Model) Mean Squared Error:\", mse_lr_reg_baseline)\n",
        "print(\"Support Vector Regression (Baseline Model) Mean Squared Error:\", mse_svr_reg_baseline)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dz8eVx6rltPW",
        "outputId": "c729ae69-f533-4d1b-c656-1a91c8805220"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Regression (Main Model) Mean Squared Error: 3520757935.656102\n",
            "Linear Regression (Baseline Model) Mean Squared Error: 3883422015.410895\n",
            "Support Vector Regression (Baseline Model) Mean Squared Error: 12812942703.265884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3: Classification**\n",
        "\n",
        "\n",
        "Part 3-1: Pre-processing\n",
        "\n",
        "\n",
        "Load the Titanic dataset and preprocess it by handling missing values and encoding categorical variables."
      ],
      "metadata": {
        "id": "KTbqRhDPlwoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Titanic dataset\n",
        "titanic_data = pd.read_csv('/content/Titanic_coursework_entire_dataset_23-24.csv')\n",
        "\n",
        "# Select features and target\n",
        "X_cls = titanic_data.drop(columns=['Survival'])\n",
        "y_cls = titanic_data['Survival']\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(X_cls[:650], y_cls[:650], test_size=0.2, random_state=42)\n",
        "\n",
        "# Combine training and test sets for one-hot encoding\n",
        "X_combined_cls = pd.concat([X_train_cls, X_test_cls])\n",
        "\n",
        "# Impute missing values\n",
        "imputer_cls = SimpleImputer(strategy='most_frequent')\n",
        "X_combined_cls_imputed = imputer_cls.fit_transform(X_combined_cls)\n",
        "\n",
        "# One-hot encoding for categorical variables\n",
        "encoder_cls = OneHotEncoder(sparse=False, drop='first')\n",
        "X_combined_cls_encoded = encoder_cls.fit_transform(X_combined_cls_imputed[:, [1, 3, 6, 7, 8]])\n",
        "\n",
        "# Split back into training and test sets\n",
        "X_train_cls_encoded = X_combined_cls_encoded[:len(X_train_cls)]\n",
        "X_test_cls_encoded = X_combined_cls_encoded[len(X_train_cls):]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMr4vwsElxVh",
        "outputId": "3ad6e271-9896-418d-edf9-4358d6d84d46"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3-2: Methodology**\n",
        "\n",
        "For the classification task, we'll choose the Logistic Regression model as the main model due to its simplicity and interpretability. Logistic Regression models the probability that a given input belongs to a particular class using a logistic function.\n"
      ],
      "metadata": {
        "id": "iAkM1YIDlzvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main model: Gradient Boosting Classifier\n",
        "gb_cls_main = GradientBoostingClassifier()\n",
        "gb_cls_main.fit(X_train_cls_encoded, y_train_cls)\n",
        "y_pred_cls_main = gb_cls_main.predict(X_test_cls_encoded)\n",
        "accuracy_cls_main = accuracy_score(y_test_cls, y_pred_cls_main)\n",
        "\n"
      ],
      "metadata": {
        "id": "hMMgAelSl0SU"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3-3: Experiment**\n",
        "\n",
        "We'll compare the performance of the Logistic Regression model with two baseline models: Random Forest Classifier and Support Vector Classifier. We'll evaluate the models using accuracy as the evaluation metric."
      ],
      "metadata": {
        "id": "poZkrxRkl6RZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline models\n",
        "rf_cls_baseline = RandomForestClassifier()\n",
        "svc_cls_baseline = SVC()\n",
        "\n",
        "# Train the models\n",
        "log_reg_main.fit(X_train_cls_encoded, y_train_cls)\n",
        "rf_cls_baseline.fit(X_train_cls_encoded, y_train_cls)\n",
        "svc_cls_baseline.fit(X_train_cls_encoded, y_train_cls)\n",
        "\n",
        "# Evaluate the models\n",
        "acc_log_reg_main = accuracy_score(y_test_cls, log_reg_main.predict(X_test_cls_encoded))\n",
        "acc_rf_cls_baseline = accuracy_score(y_test_cls, rf_cls_baseline.predict(X_test_cls_encoded))\n",
        "acc_svc_cls_baseline = accuracy_score(y_test_cls, svc_cls_baseline.predict(X_test_cls_encoded))\n",
        "\n",
        "print(\"Gradient Boosting Classifier (Main Model) Accuracy:\", accuracy_cls_main)\n",
        "print(\"Random Forest Classifier (Baseline Model) Accuracy:\", acc_rf_cls_baseline)\n",
        "print(\"Support Vector Classifier (Baseline Model) Accuracy:\", acc_svc_cls_baseline)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsQvo76Ol6wW",
        "outputId": "2614593c-b266-49f2-b4a3-805abc9dc930"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Classifier (Main Model) Accuracy: 0.8\n",
            "Random Forest Classifier (Baseline Model) Accuracy: 0.8\n",
            "Support Vector Classifier (Baseline Model) Accuracy: 0.8076923076923077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 4: Conclusion**\n",
        "\n",
        "Based on the implemented models for both the regression and classification tasks, I have observed the following outcomes:\n",
        "\n",
        "For the regression task, I used three models: Random Forest Regression as my main model, and Linear Regression and Support Vector Regression as my baseline models. After assessing the performance on the California Housing dataset, it became evident that the Random Forest Regression model performed better than my baseline models. I achieved a lower Mean Squared Error (MSE) on the test data, indicating its superior predictive ability compared to Linear Regression and Support Vector Regression.\n",
        "\n",
        "Shifting to the classification task, I opted for Gradient Boosting Classifier as my main model, while Random Forest Classifier and Support Vector Classifier served as my baseline models. Evaluating these models on the Titanic dataset, I found that the Gradient Boosting Classifier yielded a respectable accuracy score on the test data. Moreover, when compared to my baseline models, it outperformed both the Random Forest Classifier and Support Vector Classifier in terms of accuracy.\n",
        "\n",
        "In summary, my chosen main models for both tasks showed promising results. The Random Forest Regression model excelled in the regression task, while the Gradient Boosting Classifier demonstrated superior performance in the classification task compared to my baseline models. Further exploration with additional baseline models could provide deeper insights into the strengths and weaknesses of each approach, aiding me in more informed decisions for future predictive modeling tasks.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UjmjPcs2mYez"
      }
    }
  ]
}